# ğŸ¤ TOM v3.0 - Zwei Audio-Wege Definition

## ğŸ¯ **Ãœbersicht: Zwei parallele Audio-Pipelines**

Wir implementieren **zwei verschiedene Audio-Wege** fÃ¼r verschiedene Use Cases:

---

## ğŸš€ **Weg 1: WhisperX-Pipeline (High-Quality STT)**

### **Ziel:** 
Echte Spracherkennung mit GPU-Beschleunigung fÃ¼r **hochqualitative Transkription**

### **Architektur:**
```
Frontend â†’ Audio-Chunks â†’ Buffer â†’ WAV-Datei â†’ WhisperX (GPU) â†’ Text â†’ Ollama â†’ Antwort
```

### **Komponenten:**
- **STT:** WhisperX `large-v2` (CUDA 12.1, RTX 4080)
- **LLM:** Ollama `qwen3:14b` (MoE-Architektur)
- **TTS:** Piper (lokale deutsche Stimme)
- **Audio-Format:** WAV-Dateien (16kHz, Mono, 16-bit)

### **Eigenschaften:**
- âœ… **Echte Spracherkennung** (keine Mock-Texte)
- âœ… **GPU-beschleunigt** (RTX 4080)
- âœ… **Hohe QualitÃ¤t** (WhisperX large-v2)
- âœ… **Deutsche Sprache** (`language="de"`)
- â±ï¸ **Latenz:** ~2-4 Sekunden (Batch-Verarbeitung)
- ğŸ“ **Audio-Speicher:** TemporÃ¤re WAV-Dateien in `data/audio/`

### **Use Case:**
- **QualitÃ¤ts-fokussierte** Anwendungen
- **Offline-Verarbeitung** von Audio-Segmenten
- **Batch-Transkription** mit hoher Genauigkeit

---

## âš¡ **Weg 2: Realtime-Pipeline (Low-Latency)**

### **Ziel:** 
Echtzeit-Audio-Verarbeitung fÃ¼r **interaktive GesprÃ¤che** mit minimaler Latenz

### **Architektur:**
```
Frontend â†’ Audio-Stream â†’ VAD â†’ Streaming-STT â†’ Streaming-LLM â†’ Streaming-TTS â†’ Audio-Output
```

### **Komponenten:**
- **STT:** Streaming-fÃ¤hige STT (z.B. Azure Speech, OpenAI Whisper API)
- **LLM:** Streaming LLM (Token-by-Token)
- **TTS:** Streaming TTS (Chunk-by-Chunk)
- **Audio-Format:** PCM-Stream (16kHz, Real-time)

### **Eigenschaften:**
- âš¡ **Ultra-low Latency** (<500ms)
- ğŸ”„ **Echtzeit-Streaming** (keine Batch-Verarbeitung)
- ğŸ¯ **Interaktiv** (natÃ¼rliche GesprÃ¤che)
- ğŸ“¡ **Cloud-basiert** (oder lokale Streaming-APIs)
- ğŸ’° **Kosten-optimiert** (Pay-per-use)

### **Use Case:**
- **Live-Telefonie** (echte GesprÃ¤che)
- **Interaktive Assistenten**
- **Real-time Customer Service**

---

## ğŸ”„ **Parallele Implementierung**

### **Server-Konfiguration:**
```python
# apps/realtime/config.py
class RealtimeConfig:
    # Weg 1: WhisperX (High-Quality)
    REALTIME_STT_WHISPERX = RealtimeMode.LOCAL
    WHISPERX_MODEL = "large-v2"
    WHISPERX_DEVICE = "cuda"
    
    # Weg 2: Realtime (Low-Latency)
    REALTIME_STT_STREAMING = RealtimeMode.PROVIDER
    STREAMING_PROVIDER = "azure"  # oder "openai"
    STREAMING_LATENCY_TARGET = 200  # ms
```

### **Frontend-Auswahl:**
```typescript
// web/dashboard/src/components/AudioModeSelector.tsx
interface AudioMode {
  mode: 'whisperx' | 'realtime';
  description: string;
  latency: string;
  quality: 'high' | 'ultra-low';
}
```

---

## ğŸ“Š **Vergleich der Wege**

| Eigenschaft | WhisperX-Pipeline | Realtime-Pipeline |
|-------------|-------------------|-------------------|
| **Latenz** | 2-4 Sekunden | <500ms |
| **QualitÃ¤t** | Sehr hoch | Gut |
| **Kosten** | Lokal (GPU) | Cloud (Pay-per-use) |
| **Offline** | âœ… Ja | âŒ Nein |
| **Interaktiv** | âŒ Nein | âœ… Ja |
| **Use Case** | Batch-Verarbeitung | Live-GesprÃ¤che |

---

## ğŸ¯ **Aktuelle Implementierung**

### **Status WhisperX-Pipeline:**
- âœ… **Server:** `fast_server.py` (Port 8081)
- âœ… **Audio-Bufferung:** WAV-Dateien in `data/audio/`
- âœ… **WhisperX:** CUDA 12.1, RTX 4080
- âœ… **Frontend:** `better.html` (Port 3003)
- ğŸ”„ **In Arbeit:** PyTorch-CUDA Installation

### **Status Realtime-Pipeline:**
- ğŸ“‹ **Geplant:** Streaming-STT Integration
- ğŸ“‹ **Geplant:** Azure Speech Services
- ğŸ“‹ **Geplant:** Ultra-low Latency Optimierung

---

## ğŸš€ **NÃ¤chste Schritte**

1. **WhisperX-Pipeline fertigstellen** (PyTorch-Installation)
2. **Realtime-Pipeline implementieren** (Azure/OpenAI Integration)
3. **Frontend-Modus-Auswahl** hinzufÃ¼gen
4. **Performance-Messungen** durchfÃ¼hren
5. **A/B-Testing** zwischen beiden Wegen

---

**Ziel:** Beide Wege parallel verfÃ¼gbar fÃ¼r verschiedene AnwendungsfÃ¤lle! ğŸ¤âœ¨
